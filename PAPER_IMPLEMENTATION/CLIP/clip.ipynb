{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from torchtune.modules import RMSNorm\n",
    "from tokenizers import Tokenizer\n",
    "from pathlib import Path\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, Normalize, ToTensor\n",
    "from torchvision.transforms.v2 import RGB\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import random_split\n",
    "from PIL import Image\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification, ViTFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    #Hyperparameters\n",
    "    img_size = (224, 224)\n",
    "    block_size = 77\n",
    "    batch_size = 32\n",
    "    embeddings_dims = 768\n",
    "    projection_dims = 768\n",
    "    attn_dropout = 0.1\n",
    "    no_of_heads = 12 #IMP needs to be thoroughly calculated\n",
    "    dropout = 0.1\n",
    "    epochs = 100\n",
    "    lr = 4e-4\n",
    "    no_of_decoder_layers = 12 #IMP needs to be thoroughly calculated\n",
    "    weight_decay_optim = 0.2\n",
    "    beta_1 = 0.9\n",
    "    beta_2 = 0.98\n",
    "    epsilon = 1e-6\n",
    "    device = 'cuda'\n",
    "    vocab_size = 2000\n",
    "    head_lr = 1e-3\n",
    "    image_encoder_lr = 1e-4\n",
    "    text_encoder_lr = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims\n",
    "    ):  \n",
    "        super().__init__()\n",
    "        self.layernorm_layer = torch.nn.LayerNorm(normalized_shape=embeddings_dims)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.layernorm_layer(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvrajsingh/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "            \n",
    "        \n",
    "        \n",
    "        self.layer_norm = Normalization()\n",
    "        self.model = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.tokenizer = tokenizer\n",
    "        self.multimodalTextLayerProjector = nn.Linear(in_features=ModelArgs.embeddings_dims, out_features=ModelArgs.projection_dims, device=ModelArgs.device)\n",
    "        \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = True\n",
    "        self.model.train()\n",
    "    def forward(self, x):\n",
    "        # print(\"Problemetic x shape: \", x['input_ids'].shape)\n",
    "        # print(\"Problemetic x shape: \", x['attention_mask'].shape)\n",
    "        x['input_ids'] = x['input_ids'].squeeze(1)\n",
    "        x['attention_mask'] = x['attention_mask'].squeeze(1) \n",
    "        x = self.model(input_ids = x['input_ids'], attention_mask = x['attention_mask'])['last_hidden_state'][:, 0, :] \n",
    "        # print(x)\n",
    "        x = self.layer_norm(x)\n",
    "        return self.multimodalTextLayerProjector(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "            \n",
    "\n",
    "        self.multimodalVisionLayerProjector = nn.Linear(in_features=151296, out_features=ModelArgs.projection_dims, device=ModelArgs.device)\n",
    "        # self.processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', output_hidden_states=True)\n",
    "        # Initialize the feature extractor\n",
    "        self.feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = True\n",
    "        self.model.train()    \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x['image'],return_tensors=\"pt\")\n",
    "        x = x.to(ModelArgs.device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x = self.model(**x)\n",
    "            x = x.hidden_states[-1]\n",
    "            x = self.main(x)\n",
    "            # print(x)\n",
    "            # print(x.shape)\n",
    "            return self.multimodalVisionLayerProjector(x)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLiP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vision = VisionModel()\n",
    "        self.text = TextModel()\n",
    "        # self.tokenizer = tokenizer\n",
    "        self.multimodelTextLayerPorjector = nn.Linear(in_features=ModelArgs.embeddings_dims, out_features=ModelArgs.projection_dims, device=ModelArgs.device)\n",
    "        self.multimodalVisionLayerProjector = nn.Linear(in_features=ModelArgs.embeddings_dims, out_features=ModelArgs.projection_dims, device=ModelArgs.device)\n",
    "        # self.temperature = nn.Parameter(torch.ones(size=(ModelArgs.batch_size,), device=ModelArgs.device), requires_grad=True)\n",
    "        self.temperature = nn.Parameter(torch.ones([], requires_grad=True), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        embeds_text = self.text(x)\n",
    "        # print(\"Inside CLiP text: \", embeds_text.shape)\n",
    "        proj_txt = torch.nn.functional.normalize(self.multimodelTextLayerPorjector(embeds_text))\n",
    "        embeds_img = self.vision(x)\n",
    "        # print(\"Inside ViT: \", embeds_img.shape)\n",
    "        proj_img = torch.nn.functional.normalize(self.multimodalVisionLayerProjector(embeds_img))\n",
    "        # print(proj_txt.shape)\n",
    "        # print(proj_img.shape)\n",
    "        logits = (proj_txt @ proj_img.T) * torch.exp(self.temperature)\n",
    "        # print(\"Inside CLiP logits shape: \", logits.shape)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvrajsingh/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "clip = CLiP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config\n",
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "train_transforms = A.Compose(\n",
    "    [   \n",
    "        A.Resize(height=224, width=224),\n",
    "        A.CenterCrop(height=224, width=224),\n",
    "        # A.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711], max_pixel_value=224.0,),\n",
    "        # A.ToFloat(max_value=224),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_tyransforms = A.Compose(\n",
    "    [\n",
    "        # A.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711], max_pixel_value=224.0,),\n",
    "        # A.ToFloat(max_value=224),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40450</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A man in a pink shirt climbs a rock face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40451</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A man is rock climbing high in the air .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40452</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A person in a red shirt climbing up a rock fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40453</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A rock climber in a red shirt .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40454</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A rock climber practices on a rock climbing wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40455 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image  \\\n",
       "0      1000268201_693b08cb0e.jpg   \n",
       "1      1000268201_693b08cb0e.jpg   \n",
       "2      1000268201_693b08cb0e.jpg   \n",
       "3      1000268201_693b08cb0e.jpg   \n",
       "4      1000268201_693b08cb0e.jpg   \n",
       "...                          ...   \n",
       "40450   997722733_0cb5439472.jpg   \n",
       "40451   997722733_0cb5439472.jpg   \n",
       "40452   997722733_0cb5439472.jpg   \n",
       "40453   997722733_0cb5439472.jpg   \n",
       "40454   997722733_0cb5439472.jpg   \n",
       "\n",
       "                                                 caption  \n",
       "0      A child in a pink dress is climbing up a set o...  \n",
       "1                  A girl going into a wooden building .  \n",
       "2       A little girl climbing into a wooden playhouse .  \n",
       "3      A little girl climbing the stairs to her playh...  \n",
       "4      A little girl in a pink dress going into a woo...  \n",
       "...                                                  ...  \n",
       "40450           A man in a pink shirt climbs a rock face  \n",
       "40451           A man is rock climbing high in the air .  \n",
       "40452  A person in a red shirt climbing up a rock fac...  \n",
       "40453                    A rock climber in a red shirt .  \n",
       "40454  A rock climber practices on a rock climbing wa...  \n",
       "\n",
       "[40455 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/flickr8000/captions.txt', sep=',')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17775</th>\n",
       "      <td>2973269132_252bfd0160.jpg</td>\n",
       "      <td>A large wild cat is pursuing a horse across a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13506</th>\n",
       "      <td>270263570_3160f360d3.jpg</td>\n",
       "      <td>Two brown dogs fight on the leafy ground .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4325</th>\n",
       "      <td>2053006423_6adf69ca67.jpg</td>\n",
       "      <td>A man in shorts is standing on a rock looking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37870</th>\n",
       "      <td>512101751_05a6d93e19.jpg</td>\n",
       "      <td>a muzzled white dog is running on the grass .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21321</th>\n",
       "      <td>3156406419_38fbd52007.jpg</td>\n",
       "      <td>A person skiing downhill .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35640</th>\n",
       "      <td>391020801_aaaae1e42b.jpg</td>\n",
       "      <td>A man gesticulates .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12364</th>\n",
       "      <td>2629027962_9cc3b46527.jpg</td>\n",
       "      <td>With a barn in the background a child puts her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17672</th>\n",
       "      <td>2966552760_e65b22cd26.jpg</td>\n",
       "      <td>A smiling child sits against a wall on a blank...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24614</th>\n",
       "      <td>3290105461_7590f23371.jpg</td>\n",
       "      <td>Cricket player with red cap hits the ball outd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8763</th>\n",
       "      <td>2410562803_56ec09f41c.jpg</td>\n",
       "      <td>Three people sitting in front of a store on a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>405 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image  \\\n",
       "17775  2973269132_252bfd0160.jpg   \n",
       "13506   270263570_3160f360d3.jpg   \n",
       "4325   2053006423_6adf69ca67.jpg   \n",
       "37870   512101751_05a6d93e19.jpg   \n",
       "21321  3156406419_38fbd52007.jpg   \n",
       "...                          ...   \n",
       "35640   391020801_aaaae1e42b.jpg   \n",
       "12364  2629027962_9cc3b46527.jpg   \n",
       "17672  2966552760_e65b22cd26.jpg   \n",
       "24614  3290105461_7590f23371.jpg   \n",
       "8763   2410562803_56ec09f41c.jpg   \n",
       "\n",
       "                                                 caption  \n",
       "17775  A large wild cat is pursuing a horse across a ...  \n",
       "13506         Two brown dogs fight on the leafy ground .  \n",
       "4325   A man in shorts is standing on a rock looking ...  \n",
       "37870      a muzzled white dog is running on the grass .  \n",
       "21321                         A person skiing downhill .  \n",
       "...                                                  ...  \n",
       "35640                               A man gesticulates .  \n",
       "12364  With a barn in the background a child puts her...  \n",
       "17672  A smiling child sits against a wall on a blank...  \n",
       "24614  Cricket player with red cap hits the ball outd...  \n",
       "8763   Three people sitting in front of a store on a ...  \n",
       "\n",
       "[405 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sampled = df.sample(frac=0.01, random_state=42)\n",
    "df_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "class CLiPDatatset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.path = path\n",
    "        # self.dir = os.listdir(self.path)        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return df_sampled.shape[0]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        text, img = df_sampled.iloc[idx][1], df_sampled.iloc[idx][0]\n",
    "        # print(text)\n",
    "        # print(img)\n",
    "        img_path = os.path.join(self.path, img) \n",
    "        # print(img_path)\n",
    "        img = np.array(Image.open(img_path))\n",
    "\n",
    "        input_transformed = train_transforms(image = img)['image']\n",
    "        \n",
    "        text_tokenized = self.tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=ModelArgs.block_size)\n",
    "        \n",
    "        # print(text_tokenized)\n",
    "        encoded_items = {\n",
    "            \n",
    "            key: torch.tensor(values)\n",
    "            for key, values in text_tokenized.items()\n",
    "            \n",
    "        }\n",
    "        encoded_items['image'] = input_transformed\n",
    "        return encoded_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'data/flickr8000/images'\n",
    "dataset = CLiPDatatset(dir)\n",
    "\n",
    "# Assuming 'dataset' is already created\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "#Creating dataloaders\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=ModelArgs.batch_size, shuffle=True)\n",
    "valloader = DataLoader(val_dataset, batch_size=ModelArgs.batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "params = [\n",
    "        {\"params\": clip.vision.parameters(), \"lr\": ModelArgs.image_encoder_lr},\n",
    "        {\"params\": clip.text.parameters(), \"lr\": ModelArgs.text_encoder_lr},\n",
    "        {\"params\": itertools.chain(\n",
    "            clip.multimodalVisionLayerProjector.parameters(), clip.multimodelTextLayerPorjector.parameters(), [clip.temperature]\n",
    "        ), \"lr\": ModelArgs.head_lr, \"weight_decay\": ModelArgs.weight_decay_optim}\n",
    "    ]\n",
    "\n",
    "optimizer = torch.optim.Adam(lr=ModelArgs.lr, params=params, eps=ModelArgs.epsilon)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# def cross_entropy(pred=None, targets=None, dim=None):\n",
    "#     # print(\"Targets shape is: \",targets.shape)\n",
    "#     # print(\"Predictions shape is :\", pred.shape)\n",
    "    \n",
    "#     preds = nn.functional.log_softmax(pred, dim=-1)\n",
    "\n",
    "#     l = (-targets * preds).sum(1).mean()\n",
    "#     return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular import engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd43c3e5a9c4b01ad5d89f86cb5a56a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5771/697776843.py:14: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text, img = df_sampled.iloc[idx][1], df_sampled.iloc[idx][0]\n",
      "/tmp/ipykernel_5771/697776843.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  key: torch.tensor(values)\n",
      "/mnt/c/Users/Yuvraj Singh/OneDrive/Desktop/Work/pytorch/Paper Replications/CLiP/going_modular/engine.py:53: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3675.)\n",
      "  loss_t = torch.nn.functional.cross_entropy(y_pred.T, labels.T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 3.3857 | test_loss: 2.8319 \n",
      "Epoch: 2 | train_loss: 3.3842 | test_loss: 2.8363 \n",
      "Epoch: 3 | train_loss: 3.3898 | test_loss: 2.8558 \n",
      "Epoch: 4 | train_loss: 3.2075 | test_loss: 2.5720 \n",
      "Epoch: 5 | train_loss: 2.7535 | test_loss: 2.3091 \n",
      "Epoch: 6 | train_loss: 2.4067 | test_loss: 2.0960 \n",
      "Epoch: 7 | train_loss: 2.2730 | test_loss: 2.2074 \n",
      "Epoch: 8 | train_loss: 2.2631 | test_loss: 2.1497 \n",
      "Epoch: 9 | train_loss: 2.2433 | test_loss: 2.1711 \n",
      "Epoch: 10 | train_loss: 2.1443 | test_loss: 2.0108 \n",
      "Epoch: 11 | train_loss: 2.1279 | test_loss: 2.1695 \n",
      "Epoch: 12 | train_loss: 2.0682 | test_loss: 2.1878 \n",
      "Epoch: 13 | train_loss: 2.0128 | test_loss: 2.0808 \n",
      "Epoch: 14 | train_loss: 1.9314 | test_loss: 2.1472 \n",
      "Epoch: 15 | train_loss: 1.9334 | test_loss: 2.0239 \n",
      "Epoch: 16 | train_loss: 1.8983 | test_loss: 2.0606 \n",
      "Epoch: 17 | train_loss: 1.7936 | test_loss: 2.0642 \n",
      "Epoch: 18 | train_loss: 1.8178 | test_loss: 2.1490 \n",
      "Epoch: 19 | train_loss: 1.8154 | test_loss: 2.1520 \n",
      "Epoch: 20 | train_loss: 1.6850 | test_loss: 2.0929 \n",
      "Epoch: 21 | train_loss: 1.6462 | test_loss: 2.1315 \n",
      "Epoch: 22 | train_loss: 1.6416 | test_loss: 1.9998 \n",
      "Epoch: 23 | train_loss: 1.6371 | test_loss: 2.3750 \n",
      "Epoch: 24 | train_loss: 1.5850 | test_loss: 2.0301 \n",
      "Epoch: 25 | train_loss: 1.5408 | test_loss: 2.0466 \n",
      "Epoch: 26 | train_loss: 1.4817 | test_loss: 2.0330 \n",
      "Epoch: 27 | train_loss: 1.4636 | test_loss: 2.1185 \n",
      "Epoch: 28 | train_loss: 1.4670 | test_loss: 2.0971 \n",
      "Epoch: 29 | train_loss: 1.4414 | test_loss: 2.0514 \n",
      "Epoch: 30 | train_loss: 1.3791 | test_loss: 2.2192 \n"
     ]
    }
   ],
   "source": [
    "results = engine.train(model=clip,\n",
    "                       writer=None,\n",
    "                       train_dataloader=trainloader,\n",
    "                       test_dataloader=valloader,\n",
    "                       optimizer=optimizer,\n",
    "                       loss_fn=loss_fn,\n",
    "                       epochs=30,\n",
    "                       device=ModelArgs.device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
